!pip install transformers datasets torch

pip install --upgrade transformers

pip install -U transformers[torch] accelerate

from transformers.training_args import TrainingArguments

# Create a text file with sample data
sample_text = """This is the first sentence.
GPT-2 is a transformer-based model.
Fine-tuning helps in generating better text.
Machine learning is fascinating!
AI can be used for multiple applications."""

# Save to a file
with open("custom_data.txt", "w") as f:
    f.write(sample_text)

print("Dataset created successfully!")

from datasets import load_dataset

# Load the dataset from the text file
dataset = load_dataset("text", data_files={"train": "custom_data.txt"})

# Print dataset to verify
print(dataset)

from transformers import GPT2Tokenizer

# Load GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Ensure tokenizer uses an EOS token
tokenizer.pad_token = tokenizer.eos_token

# Tokenization function
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

# Apply tokenization
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Print tokenized data
print(tokenized_datasets)

import torch

# Convert dataset to PyTorch format
tokenized_datasets.set_format(type="torch", columns=["input_ids", "attention_mask"])

# Create train dataset
train_dataset = tokenized_datasets["train"]

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",
    eval_strategy="epoch",  # Updated from evaluation_strategy
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=1,
)

